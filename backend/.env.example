# ====================
# 后端环境变量示例
# ====================
# 说明：
# - 请复制为 backend/.env 并按需修改
# - 不要提交真实的密钥/Token 到仓库

# ====================
# 基础配置
# ====================
# 运行环境：development / test / production
NODE_ENV=development

# 后端监听端口（backend/src/main.ts 会读取 PORT）
PORT=5181

# API 路由前缀（例如 /api）
API_PREFIX=api

# CORS 允许来源，开发阶段可使用 *，生产建议填写前端域名
CORS_ORIGIN=*

# ====================
# PostgreSQL（Prisma）
# ====================
# Prisma 默认读取 DATABASE_URL
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/meeting_ai

# 兼容字段：ConfigurationService / configuration.ts 读取 postgres.url
POSTGRES_URL=postgresql://postgres:postgres@localhost:5432/meeting_ai

# ====================
# MongoDB（Mongoose）
# ====================
# 运行时实际连接字段：backend/src/database/mongodb.module.ts 读取 MONGODB_URI
MONGODB_URI=mongodb://localhost:27017/meeting_ai

# 兼容字段：ConfigurationService / configuration.ts 读取 mongo.url
MONGODB_URL=mongodb://localhost:27017/meeting_ai

# ====================
# AI 模型配置
# ====================
# GLM（智谱）Chat Completions（backend/src/modules/analysis/clients/glm.client.ts 读取 GLM_API_KEY）
GLM_API_KEY=your_glm_api_key

# 可选：自定义 GLM Endpoint（ConfigurationService 会读取 ai.glm.endpoint）
GLM_ENDPOINT=https://open.bigmodel.cn/api/paas/v4/chat/completions

# GLM 分析模型名称
GLM_ANALYSIS_MODEL=glm-4xx

# ====================
# 实时转写（ASR）
# ====================
# GLM-ASR-2512：HTTP + SSE 流式返回，需要累积音频后发送

# 转写管线开关：legacy（默认）/ raw_llm（并行写入原文事件流，后续用于 LLM 分段）
TRANSCRIPT_PIPELINE=legacy

# ====================
# 语句拆分（transcript_events_segments）
# ====================
# 去抖间隔（毫秒）；0 表示每次事件更新都立即触发（不建议）
TRANSCRIPT_EVENTS_SEGMENT_INTERVAL_MS=3000

# 上下文窗口（CHUNK_SIZE）：取尾部 N 条事件作为 LLM 上下文
TRANSCRIPT_EVENTS_SEGMENT_CHUNK_SIZE=120
# 兼容别名（历史字段）：WINDOW_EVENTS 也作为窗口大小配置
# TRANSCRIPT_EVENTS_SEGMENT_WINDOW_EVENTS=120

# 收到 end_turn 时立即触发一次语句拆分
TRANSCRIPT_EVENTS_SEGMENT_TRIGGER_ON_END_TURN=1

# 收到 stop_transcribe 时立即触发一次语句拆分
TRANSCRIPT_EVENTS_SEGMENT_TRIGGER_ON_STOP_TRANSCRIBE=1

# 单次语句拆分最大生成段数
TRANSCRIPT_EVENTS_SEGMENT_MAX_SEGMENTS_PER_RUN=8

# 语句拆分使用的 GLM 模型（默认 glm-4xxx）
GLM_TRANSCRIPT_EVENT_SEGMENT_MODEL=glm-4XX

# 语句拆分：GLM 输出上限（tokens）
GLM_TRANSCRIPT_EVENT_SEGMENT_MAX_TOKENS=2000

# 语句拆分：GLM 输出上限补偿（tokens）
GLM_TRANSCRIPT_EVENT_SEGMENT_BUMP_MAX_TOKENS=4096

# 语句拆分：JSON 模式（1/true 启用，0/false 关闭）
GLM_TRANSCRIPT_EVENT_SEGMENT_JSON_MODE=1

# 语句拆分：GLM 429 限流重试配置
# 最大重试次数（0 表示不重试）
GLM_TRANSCRIPT_EVENT_SEGMENT_RETRY_MAX=2
# 退避基准延迟（毫秒）
GLM_TRANSCRIPT_EVENT_SEGMENT_RETRY_BASE_MS=500
# 退避最大延迟（毫秒）
GLM_TRANSCRIPT_EVENT_SEGMENT_RETRY_MAX_MS=8000

# 转写自动切分间隔（毫秒）
TRANSCRIPT_AUTO_SPLIT_GAP_MS=2500

# 转写调试：打印语句日志（1/true 启用，0/false 关闭）
TRANSCRIPT_DEBUG_LOG_UTTERANCES=0

# ASR 缓冲切分上限（毫秒）
# 软上限：有明显静音时优先在此切分
TRANSCRIPT_MAX_BUFFER_DURATION_SOFT_MS=30000
# 硬上限：无静音也强制切分，避免超过 ASR 限制
TRANSCRIPT_MAX_BUFFER_DURATION_HARD_MS=50000
