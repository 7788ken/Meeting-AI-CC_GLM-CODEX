# ====================
# 后端环境变量示例
# ====================
# 说明：
# - 请复制为 backend/.env 并按需修改
# - 不要提交真实的密钥/Token 到仓库

# ====================
# 基础配置
# ====================
# 运行环境：development / test / production
NODE_ENV=development

# 后端监听端口（backend/src/main.ts 会读取 PORT）
PORT=5181

# API 路由前缀（例如 /api）
API_PREFIX=api

# CORS 允许来源，开发阶段可使用 *，生产建议填写前端域名
CORS_ORIGIN=*

# ====================
# PostgreSQL（Prisma）
# ====================
# Prisma 默认读取 DATABASE_URL
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/meeting_ai

# 兼容字段：ConfigurationService / configuration.ts 读取 postgres.url
POSTGRES_URL=postgresql://postgres:postgres@localhost:5432/meeting_ai

# ====================
# MongoDB（Mongoose）
# ====================
# 运行时实际连接字段：backend/src/database/mongodb.module.ts 读取 MONGODB_URI
MONGODB_URI=mongodb://localhost:27017/meeting_ai

# 兼容字段：ConfigurationService / configuration.ts 读取 mongo.url
MONGODB_URL=mongodb://localhost:27017/meeting_ai

# ====================
# AI 模型配置
# ====================
# GLM（智谱）Chat Completions（backend/src/modules/analysis/clients/glm.client.ts 读取 GLM_API_KEY）
GLM_API_KEY=your_glm_api_key

# 可选：自定义 GLM Endpoint（ConfigurationService 会读取 ai.glm.endpoint）
GLM_ENDPOINT=https://open.bigmodel.cn/api/paas/v4/chat/completions

# GLM 分析模型名称
GLM_ANALYSIS_MODEL=glm-4.6v-flash

# 通义千问（如启用对应客户端/适配）
QIANWEN_API_KEY=your_qianwen_api_key
QIANWEN_ENDPOINT=https://dashscope.aliyuncs.com/api/v1

# 豆包大模型（如启用对应客户端/适配）
DOUBAO_API_KEY=your_doubao_api_key
DOUBAO_ENDPOINT=

# ====================
# 实时转写（ASR）
# ====================
# ASR 模型选择：doubao（豆包实时流式） / glm（智谱 GLM-ASR-2512）
# 豆包：WebSocket 实时流式，延迟极低
# 智谱：HTTP + SSE 流式返回，需要累积音频后发送
TRANSCRIPT_MODEL=doubao

# GLM（智谱AI）API Key（使用 GLM-ASR-2512 时必需）
GLM_API_KEY=your_glm_api_key

# 转写管线开关：legacy（默认）/ raw_llm（并行写入原文事件流，后续用于 LLM 分段）
TRANSCRIPT_PIPELINE=legacy

# ====================
# 语句拆分（transcript_events_segments）
# ====================
# 去抖间隔（毫秒）；0 表示每次事件更新都立即触发（不建议）
TRANSCRIPT_EVENTS_SEGMENT_INTERVAL_MS=3000

# 上下文窗口（CHUNK_SIZE）：取尾部 N 条事件作为 LLM 上下文
TRANSCRIPT_EVENTS_SEGMENT_CHUNK_SIZE=120
# 兼容别名（历史字段）：WINDOW_EVENTS 也作为窗口大小配置
# TRANSCRIPT_EVENTS_SEGMENT_WINDOW_EVENTS=120

# 收到 end_turn 时立即触发一次语句拆分
TRANSCRIPT_EVENTS_SEGMENT_TRIGGER_ON_END_TURN=1

# 收到 stop_transcribe 时立即触发一次语句拆分
TRANSCRIPT_EVENTS_SEGMENT_TRIGGER_ON_STOP_TRANSCRIBE=1

# 语句拆分使用的 GLM 模型（默认 glm-4.6v-flash）
GLM_TRANSCRIPT_EVENT_SEGMENT_MODEL=glm-4.6v-flash

# 语义分析使用的 GLM 模型（transcript_analysis）
GLM_TRANSCRIPT_ANALYSIS_MODEL=glm-4.6v-flash

# 语句拆分：GLM 输出上限（tokens）
GLM_TRANSCRIPT_EVENT_SEGMENT_MAX_TOKENS=2000

# 语句拆分：JSON 模式（1/true 启用，0/false 关闭）
GLM_TRANSCRIPT_EVENT_SEGMENT_JSON_MODE=1

# 兼容字段：ConfigurationService 读取 transcript.apiKey（如你实现基于 API Key 的转写服务）
TRANSCRIPT_API_KEY=

# 豆包 ASR 鉴权配置（使用 doubao 模式时必需）
TRANSCRIPT_ENDPOINT=wss://openspeech.bytedance.com/api/v3/sauc/bigmodel_async
TRANSCRIPT_APP_KEY=your_transcript_app_key
TRANSCRIPT_ACCESS_KEY=your_transcript_access_key
TRANSCRIPT_RESOURCE_ID=volc.bigasr.sauc.duration

# ASR 缓冲切分上限（毫秒）
# 软上限：有明显静音时优先在此切分
TRANSCRIPT_MAX_BUFFER_DURATION_SOFT_MS=30000
# 硬上限：无静音也强制切分，避免超过 ASR 限制
TRANSCRIPT_MAX_BUFFER_DURATION_HARD_MS=50000

# 可选：等待 ASR 响应的超时时间（毫秒），默认 5000
TRANSCRIPT_RESPONSE_TIMEOUT_MS=5000

# 可选：Config 是否使用 gzip（默认 true；若服务端不接受可设为 false）
TRANSCRIPT_CONFIG_GZIP=true

# 可选：音频包是否使用 gzip（默认 true；对齐 Java demo；若服务端不接受可设为 false）
TRANSCRIPT_AUDIO_GZIP=true
